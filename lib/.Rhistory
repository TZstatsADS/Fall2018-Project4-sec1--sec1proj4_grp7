validation <- (1:100)[! 1:100 %in% training]
validation
files <- list.files(path="../data/ground_truth/", pattern="*.txt", full.names=TRUE, recursive=FALSE)
truthFiles <- list.files(path=truthPath, pattern="*.txt", full.names=TRUE, recursive=FALSE)
ocrFiles <- list.files(path=ocrPath, pattern="*.txt", full.names=TRUE, recursive=FALSE)
truthPath = "../data/ground_truth/"
ocrPath = "../data/tesseract/"
truthFiles <- list.files(path=truthPath, pattern="*.txt", full.names=TRUE, recursive=FALSE)
ocrFiles <- list.files(path=ocrPath, pattern="*.txt", full.names=TRUE, recursive=FALSE)
length(truthFiles)
length(ocrFiles)
if(length(truthFiles) != length(ocrFiles)) stop("the number of ground truth files is not equal to the number of OCR recognized files")
if(length(truthFiles) != 100) stop("the number of ground truth files is not equal to the number of OCR recognized files")
if(length(truthFiles) != 200) stop("the number of ground truth files is not equal to the number of OCR recognized files")
library(stringr)
library(tm)
library(dplyr)
library(tidytext)
library(broom)
truthPath = "../data/ground_truth/"
ocrPath = "../data/tesseract/"
truthFiles <- list.files(path=truthPath, pattern="*.txt", full.names=TRUE, recursive=FALSE)
ocrFiles <- list.files(path=ocrPath, pattern="*.txt", full.names=TRUE, recursive=FALSE)
if(length(truthFiles) != 200) stop("the number of ground truth files is not equal to the number of OCR recognized files")
0.8*n
n <- length(truthFiles) # number of files
0.8*n
n <- 101
0.8*n
round(0.8*n)
# define what to run
doDic = F # doDic=false means we can choose our own dictionary. It can be groundtruth or other online dictionary.
doDetect = F # same as above
doDigram = F
doConfusion = F # compute confusion matrix
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
library(stringr)
library(tm)
library(dplyr)
library(tidytext)
library(broom)
# read files
truthPath = "../data/ground_truth/"
ocrPath = "../data/tesseract/"
truthFiles <- list.files(path=truthPath, pattern="*.txt", full.names=TRUE, recursive=FALSE)
ocrFiles <- list.files(path=ocrPath, pattern="*.txt", full.names=TRUE, recursive=FALSE)
if(length(truthFiles) != length(ocrFiles)) stop("the number of ground truth files is not equal to the number of OCR recognized files")
n <- length(truthFiles) # number of files
# select train + validation and testing data 80% and 20%
set.seed(1984)
training <- sample(1:n, round(0.8*n))
testing <- (1:n)[!1:n %in% training]
truthTrain <- truthFiles[training]
ocrTrain <- ocrFiles[training]
truthTest <- truthFiles[testing]
ocrTest <- ocrFiles[testing]
# the assigned paper for error correction is C5
# step 1: train word from groundtruth with topic modeling, get the probability of each topic in each documents, probability of each word in each topic.
# step 2:
# step 3:
# source("../lib/CandidateWords.R")
# source("../lib/LDA.R")
source("../lib/confusionMatrix.R")
if(doConfusion == T) {
commonMistakes(truthTrain, ocrTrain)
} else {
load("../output/confusionMatrix.RData")
}
doConfusion = T
if(doConfusion == T) {
commonMistakes(truthTrain, ocrTrain)
} else {
load("../output/confusionMatrix.RData")
}
traceback()
# the assigned paper for error correction is C5
# step 1: train word from groundtruth with topic modeling, get the probability of each topic in each documents, probability of each word in each topic.
# step 2:
# step 3:
# source("../lib/CandidateWords.R")
# source("../lib/LDA.R")
source("../lib/confusionMatrix.R")
if(doConfusion == T) {
commonMistakes(truthTrain, ocrTrain)
} else {
load("../output/confusionMatrix.RData")
}
str(confMat)
dimnames(confMat)
# the assigned paper for error correction is C5
# step 1: train word from groundtruth with topic modeling, get the probability of each topic in each documents, probability of each word in each topic.
# step 2:
# step 3:
# source("../lib/CandidateWords.R")
# source("../lib/LDA.R")
source("../lib/confusionMatrix.R")
if(doConfusion == T) {
commonMistakes(truthTrain, ocrTrain)
} else {
load("../output/confusionMatrix.RData")
}
traceback()
# the assigned paper for error correction is C5
# step 1: train word from groundtruth with topic modeling, get the probability of each topic in each documents, probability of each word in each topic.
# step 2:
# step 3:
# source("../lib/CandidateWords.R")
# source("../lib/LDA.R")
source("../lib/confusionMatrix.R")
if(doConfusion == T) {
commonMistakes(truthTrain, ocrTrain)
} else {
load("../output/confusionMatrix.RData")
}
# the assigned paper for error correction is C5
# step 1: train word from groundtruth with topic modeling, get the probability of each topic in each documents, probability of each word in each topic.
# step 2:
# step 3:
# source("../lib/CandidateWords.R")
# source("../lib/LDA.R")
source("../lib/confusionMatrix.R")
if(doConfusion == T) {
commonMistakes(truthTrain, ocrTrain)
} else {
load("../output/confusionMatrix.RData")
}
groundTruth <- array(NA, c(1, length(truthTrain)))
for(x in 1:length(truthTrain)) groundTruth[,x] <- readChar(truthTrain[x], file.info(truthTrain[x])$size)
groundTruth <- strsplit(groundTruth,"\n")
tesseract <- array(NA, c(1, length(ocrTrain)))
for(x in 1:length(ocrTrain)) tesseract[,x] <- readChar(ocrTrain[x], file.info(ocrTrain[x])$size)
tesseract <- strsplit(tesseract,"\n")
a <- array(0, c(36, 36))
possibilities <- c(0:9, letters)
possibilities
for(i in 1:length(truthTrain)){
truthBag <- strsplit(groundTruth[[i]]," ")
tessBag <- strsplit(tesseract[[i]]," ")
for(j in 1 : min(length(truthBag), length(tessBag))){
if(length(truthBag[[j]]) == length(tessBag[[j]])){
truthWords <- truthBag[[j]]
tessWords <- tessBag[[j]]
for(k in 1: length(truthWords)){
if((nchar(truthWords[k]) == nchar(tessWords[k]))){
truthLetters <- unlist(strsplit(truthWords[k], ""))
tessLetters <- unlist(strsplit(tessWords[k], ""))
for(l in 1:length(truthLetters)){
if(truthLetters[l] != tessLetters[l]){
a[match(tolower(truthLetters[l]), possibilities),
match(tolower(tessLetters[l]), possibilities)] =
a[match(tolower(truthLetters[l]), possibilities),
match(tolower(tessLetters[l]), possibilities)] + 1
}
}
}
}
}
}
}
View(a)
confMat <- matrix(a, dimnames = list(c(possibilities, possibilities)))
possibilities
length(possibilities)
confMat <- matrix(a, dimnames = list(possibilities, possibilities))
list(possibilities, possibilities)
confMat <- matrix(a, dimnames = list(possibilities, possibilities))
matrix(a)
as.matrix(a)
confMat <- as.matrix(a, dimnames = list(possibilities, possibilities))
View(confMat)
confMat
confMat["a",]
confMat <- as.matrix(a, dimnames = list(possibilities, possibilities))
typeof(confMat)
class(confMat)
confMat["a",]
confMat[1,]
a <- matrix(0, 36, 36, dimnames = list(possibilities, possibilities))
possibilities <- c(0:9, letters)
possibilities <- c(0:9, letters)
a <- matrix(0, 36, 36, dimnames = list(possibilities, possibilities))
for(i in 1:length(truthTrain)){
truthBag <- strsplit(groundTruth[[i]]," ")
tessBag <- strsplit(tesseract[[i]]," ")
for(j in 1 : min(length(truthBag), length(tessBag))){
if(length(truthBag[[j]]) == length(tessBag[[j]])){
truthWords <- truthBag[[j]]
tessWords <- tessBag[[j]]
for(k in 1: length(truthWords)){
if((nchar(truthWords[k]) == nchar(tessWords[k]))){
truthLetters <- unlist(strsplit(truthWords[k], ""))
tessLetters <- unlist(strsplit(tessWords[k], ""))
for(l in 1:length(truthLetters)){
if(truthLetters[l] != tessLetters[l]){
a[match(tolower(truthLetters[l]), possibilities),
match(tolower(tessLetters[l]), possibilities)] =
a[match(tolower(truthLetters[l]), possibilities),
match(tolower(tessLetters[l]), possibilities)] + 1
}
}
}
}
}
}
}
View(a)
if(doConfusion == T) {
commonMistakes(truthTrain, ocrTrain)
} else {
load("../output/confusionMatrix.RData")
}
# the assigned paper for error correction is C5
# step 1: train word from groundtruth with topic modeling, get the probability of each topic in each documents, probability of each word in each topic.
# step 2:
# step 3:
# source("../lib/CandidateWords.R")
# source("../lib/LDA.R")
source("../lib/confusionMatrix.R")
if(doConfusion == T) {
commonMistakes(truthTrain, ocrTrain)
} else {
load("../output/confusionMatrix.RData")
}
View(confMat)
# the assigned paper for error correction is C5
# step 1: train word from groundtruth with topic modeling, get the probability of each topic in each documents, probability of each word in each topic.
# step 2:
# step 3:
# source("../lib/CandidateWords.R")
# source("../lib/LDA.R")
source("../lib/confusionMatrix.R")
if(doConfusion == T) {
confMat <- commonMistakes(truthTrain, ocrTrain)
} else {
load("../output/confusionMatrix.RData")
}
View(confMat)
if(doConfusion == T)
if(doConfusion == T) confMat <- commonMistakes(truthTrain, ocrTrain) else load("../output/confusionMatrix.RData")
truthTrain
a  <- Corpus(truthTrain, readerControl = list(language="lat")) #specifies the exact folder where my text file(s) is for analysis with tm.
a  <- Corpus(DirSource(truthTrain), readerControl = list(language="lat")) #specifies the exact folder where my text file(s) is for analysis with tm.
a  <- Corpus(VectorSource(truthTrain), readerControl = list(language="lat")) #specifies the exact folder where my text file(s) is for analysis with tm.
a  <- Corpus(VectorSource(truthTrain), readerControl = list(language="lat")) #specifies the exact folder where my text file(s) is for analysis with tm.
a <- tm_map(a, content_transformer(tolower))
a  <- Corpus(VectorSource(truthTrain), readerControl = list(language="lat")) #specifies the exact folder where my text file(s) is for analysis with tm.
a <- tm_map(a, content_transformer(tolower))
a <- tm_map(a , stripWhitespace)
a <- tm_map(a, removeWords, sw)
a <- tm_map(a, removeWords, stopwords("en"))
adtm <- DocumentTermMatrix(a)
l <- LDA(adtm, k = 30, control = list(seed = 1234))
library(topicmodels)
l <- LDA(adtm, k = 30, control = list(seed = 1234))
ap_topics <- tidy(l, matrix = "beta")
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
top_n(20, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
library(ggplot2)
ap_top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
beta_spread <- ap_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .001 | topic2 > .001) %>%
mutate(log_ratio = log2(topic2 / topic1))
a <- Corpus(VectorSource(truthTrain), readerControl = list(language="lat")) #specifies the exact folder where my text file(s) is for analysis with tm.
nedir <- lapply(truthTrain, function (filename) read.table(filename, sep="\t", stringsAsFactors = F))
View(nedir)
warnings()
hepsi <- lapply( nedir, function(x) x$V1)
a <- Corpus(VectorSource(hepsi), readerControl = list(language="lat")) #specifies the exact folder where my text file(s) is for analysis with tm.
a <- tm_map(a, tolower)
a <- tm_map(a , stripWhitespace)
a <- tm_map(a, removeWords, stopwords("en"))
adtm <- DocumentTermMatrix(a)
l <- LDA(adtm, k = 30, control = list(seed = 1234))
ap_topics <- tidy(l, matrix = "beta")
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
top_n(20, beta) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
topicMix <- tidy(l, matrix = "gamma")
save(l, file = "../output/LDA.RData")
wordTopic <- tidy(l, matrix = "beta")
docTopic <- tidy(l, matrix = "gamma")
load("../output/dict.RData")
load("../output/OCRText.RData")
load("../output/OCRText.RData")
d2 <- OCRText[nchar(OCRText[,1]) < 16,]
d2 <- d2[nchar(d2[,1]) > 1,] # no single character words
for(i in 1:nrow(d2)){
n <- nchar(d2[i,1])
counter <- 1
for(k in 1 : (n-1)){
for(l in 2:n){
if(k < l){
# cat(i, " ", k, " ", l)
if(a[n, counter, match(substr(d2[i,1], k, k), letters),
match(substr(d2[i,1], l, l), letters)] == 0){
d2[i,2] <- 1
stop = TRUE
break
}
counter <- counter + 1
if (stop){break}
}
if (stop){break}
}
if (stop){break}
}
}
load("../output/digrams.RData")
for(i in 1:nrow(d2)){
n <- nchar(d2[i,1])
counter <- 1
for(k in 1 : (n-1)){
for(l in 2:n){
if(k < l){
# cat(i, " ", k, " ", l)
if(a[n, counter, match(substr(d2[i,1], k, k), letters),
match(substr(d2[i,1], l, l), letters)] == 0){
d2[i,2] <- 1
stop = TRUE
break
}
counter <- counter + 1
if (stop){break}
}
if (stop){break}
}
if (stop){break}
}
}
for(i in 1:nrow(d2)){
n <- nchar(d2[i,1])
counter <- 1
for(k in 1 : (n-1)){
for(l in 2:n){
if(k < l){
# cat(i, " ", k, " ", l)
if(a[n, counter, match(substr(d2[i,1], k, k), letters),
match(substr(d2[i,1], l, l), letters)] == 0){
d2[i,2] <- 1
stopping = TRUE
break
}
counter <- counter + 1
if (stopping){break}
}
if (stopping){break}
}
if (stopping){break}
}
}
str(OCRText)
d2[i,1]
match(substr(d2[i,1], k, k)
match(substr(d2[i,1], k, k), letters)
counter
match(substr(d2[i,1], l, l), letters)
a[n, counter, match(substr(d2[i,1], k, k), letters),
match(substr(d2[i,1], l, l), letters)]
stop = F
for(i in 1:nrow(d2)){
n <- nchar(d2[i,1])
counter <- 1
for(k in 1 : (n-1)){
for(l in 2:n){
if(k < l){
# cat(i, " ", k, " ", l)
if(a[n, counter, match(substr(d2[i,1], k, k), letters),
match(substr(d2[i,1], l, l), letters)] == 0){
d2[i,2] <- 1
stop = TRUE
break
}
counter <- counter + 1
if (stop){break}
}
if (stop){break}
}
if (stop){break}
}
}
a[n, counter, match(substr(d2[i,1], k, k), letters),
match(substr(d2[i,1], l, l), letters)]
match(substr(d2[i,1], k, k)
match(substr(d2[i,1], k, k), letters)
match(substr(d2[i,1], l, l), letters)
k
d2[i,1]
tesseract <- ""
for(x in ocrTrain) tesseract <- paste(tesseract, readChar(x, file.info(x)$size))
truthPath = "../data/ground_truth/"
ocrPath = "../data/tesseract/"
truthFiles <- list.files(path=truthPath, pattern="*.txt", full.names=TRUE, recursive=FALSE)
ocrFiles <- list.files(path=ocrPath, pattern="*.txt", full.names=TRUE, recursive=FALSE)
if(length(truthFiles) != length(ocrFiles)) stop("the number of ground truth files is not equal to the number of OCR recognized files")
n <- length(truthFiles) # number of files
# select train + validation and testing data 80% and 20%
set.seed(1984)
training <- sample(1:n, round(0.8*n))
testing <- (1:n)[!1:n %in% training]
truthTrain <- truthFiles[training]
ocrTrain <- ocrFiles[training]
truthTest <- truthFiles[testing]
ocrTest <- ocrFiles[testing]
tesseract <- ""
for(x in ocrTrain) tesseract <- paste(tesseract, readChar(x, file.info(x)$size))
tesseract <- strsplit(tesseract,"\n")[[1]]
tesseract <- tesseract[tesseract!=""]
bag <- str_split(tesseract," ")
bag <- unlist(bag)
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- VCorpus(VectorSource(bag))%>%
tm_map(content_transformer(tolower))%>%
tm_map(toSpace, "\\W")%>%
tm_map(removePunctuation)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
dict2 <- tidytext::tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
dict2 <- as.matrix(dict2)
dict2
OCRText <- cbind(dict2, rep(0, nrow(dict2)))
OCRText
cbind(dict2, rep(0, nrow(dict2)))
OCRText <- cbind(dict2, as.numeric(rep(0, nrow(dict2))))
OCRText
head(OCRText)
OCRText[,2]
OCRText[,2] <- as.numeric(OCRText[,2])
OCRText[,2]
substr(d2[i,1], k, k)
d2[i,1]
as.numeric(d2[i,1])
is.na(as.numeric(d2[i,1]))
is.na(as.numeric("17"))
as.numeric("17")
d2[i,1]
i
groundTruth <- ""
for(x in truthTrain) groundTruth <- paste(groundTruth, readChar(x, file.info(x)$size))
head(groundTruth)
d2[i,1]
strsplit(d2[i,1], "")
strsplit(d2[i,1], "") %in% letters
unlist(strsplit(d2[i,1], "")) %in% letters
prod(unlist(strsplit(d2[i,1], "")) %in% letters)
prod(unlist(strsplit(abc", "")) %in% letters)
prod(unlist(strsplit("abc", "")) %in% letters)
load("../output/OCRText.RData")
d2 <- OCRText[nchar(OCRText[,1]) < 16,]
d2 <- d2[nchar(d2[,1]) > 1,] # no single character words
stop = F
for(i in 1:nrow(d2)){
n <- nchar(d2[i,1])
counter <- 1
if(prod(unlist(strsplit(d2[i,1], "")) %in% letters)){ # if contains only letters
for(k in 1 : (n-1)){
for(l in 2:n){
if(k < l){
# cat(i, " ", k, " ", l)
if(a[n, counter, match(substr(d2[i,1], k, k), letters),
match(substr(d2[i,1], l, l), letters)] == 0){
d2[i,2] <- 1
stop = TRUE
break
}
counter <- counter + 1
if (stop){break}
}
if (stop){break}
}
if (stop){break}
}
}
}
head(d2)
table(as.numeric(d2[,2]))
load("../output/LDA.RData")
wordTopic <- tidy(l, matrix = "beta")
docTopic <- tidy(l, matrix = "gamma")

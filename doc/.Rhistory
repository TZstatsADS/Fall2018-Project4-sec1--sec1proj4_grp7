if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zipcode",
export=TRUE))
}
if(!require("EBimage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
library(EBimage)
library("EBimage")
biocLite("EBImage")
library("EBimage")
library(EBimage)
library("EBImage")
setwd("./ads_spr2017_proj3")
experiment_dir <- "../data/zipcode/" # This will be modified for different data sets.
img_train_dir <- paste(experiment_dir, "train/", sep="")
img_test_dir <- paste(experiment_dir, "test/", sep="")
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
model_values <- seq(3, 11, 2)
model_labels = paste("GBM with depth =", model_values)
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep=""),
header=F)
label_train <- as.numeric(unlist(label_train) == "9")
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zipcode",
export=TRUE))
}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(img_test_dir,
"test",
data_name="zip",
export=TRUE))
#save(dat_train, file="./output/feature_train.RData")
#save(dat_test, file="./output/feature_test.RData")
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(img_test_dir,
"test",
data_name="zip",
export=TRUE))
}
#save(dat_train, file="./output/feature_train.RData")
#save(dat_test, file="./output/feature_test.RData")
source("../lib/train.R")
source("../lib/test.R")
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(depth_values), 2))
for(k in 1:length(depth_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(model_values), 2))
for(k in 1:length(depth_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
experiment_dir <- "../data/zipcode/" # This will be modified for different data sets.
img_train_dir <- paste(experiment_dir, "train/", sep="")
img_test_dir <- paste(experiment_dir, "test/", sep="")
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
model_values <- seq(3, 11, 2)
model_labels = paste("GBM with depth =", model_values)
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep=""),
header=F)
label_train <- as.numeric(unlist(label_train) == "9")
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(img_test_dir,
"test",
data_name="zip",
export=TRUE))
}
#save(dat_train, file="./output/feature_train.RData")
#save(dat_test, file="./output/feature_test.RData")
source("../lib/train.R")
source("../lib/test.R")
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
if(run.cv){
load("../output/err_cv.RData")
#pdf("../fig/cv_results.pdf", width=7, height=5)
plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
main="Cross Validation Error", type="n", ylim=c(0, 0.15))
points(model_values, err_cv[,1], col="blue", pch=16)
lines(model_values, err_cv[,1], col="blue")
arrows(model_values, err_cv[,1]-err_cv[,2],depth_values, err_cv[,1]+err_cv[,2],
length=0.1, angle=90, code=3)
#dev.off()
}
if(run.cv){
load("../output/err_cv.RData")
#pdf("../fig/cv_results.pdf", width=7, height=5)
plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
main="Cross Validation Error", type="n", ylim=c(0, 0.15))
points(model_values, err_cv[,1], col="blue", pch=16)
lines(model_values, err_cv[,1], col="blue")
arrows(model_values, err_cv[,1]-err_cv[,2], model_values, err_cv[,1]+err_cv[,2],
length=0.1, angle=90, code=3)
#dev.off()
}
if(run.cv){
load("../output/err_cv.RData")
#pdf("../fig/cv_results.pdf", width=7, height=5)
plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
main="Cross Validation Error", type="n", ylim=c(0, 0.25))
points(model_values, err_cv[,1], col="blue", pch=16)
lines(model_values, err_cv[,1], col="blue")
arrows(model_values, err_cv[,1]-err_cv[,2], model_values, err_cv[,1]+err_cv[,2],
length=0.1, angle=90, code=3)
#dev.off()
}
model_best=model_values[1]
if(run.cv){
model_best <- model_values[which.min(err_cv[,1])]
}
par_best <- list(par=model_best)
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
View(err_cv)
which.min(err_cv[,1])
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
fit_train <- train(dat_train, label_train, par_best)
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par=par_best))
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par=par_best))
par_best$par
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
source('~/Dropbox/Tian_Teaching/G5243-ADS/0-Projects-startercodes/3-Spring2017/Project3_PoodleKFC/lib/train.R')
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
source('~/Dropbox/Tian_Teaching/G5243-ADS/0-Projects-startercodes/3-Spring2017/Project3_PoodleKFC/lib/train.R')
model_best=model_values[1]
if(run.cv){
model_best <- model_values[which.min(err_cv[,1])]
}
par_best <- list(depth=model_best)
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
save(fit_train, file="../output/fit_train.RData")
tm_test=NA
if(run.test){
load(file=paste0("../output/feature_", "zip", "_", "test", ".RData"))
load(file="../output/fit_train.RData")
tm_test <- system.time(pred_test <- test(fit_train, dat_test))
save(pred_test, file="../output/pred_test.RData")
}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n")
cat("Time for making prediction=", tm_test[1], "s \n")
if (!require("pacman")) install.packages("pacman")
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
require("pacman")
install.packages("pacman")
if (!require("pacman")) {
## Make sure your current packages are up to date
update.packages()
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
if (!require("pacman")) {
## Make sure your current packages are up to date
update.packages()
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
install.packages(c("acs", "backports", "bayesplot", "BH", "bit", "bit64", "blogdown", "bookdown", "boot", "broom", "cairoDevice", "car", "caTools", "checkmate", "choroplethr", "chron", "cli", "cluster", "coin", "colourpicker", "countrycode", "curl", "data.table", "dendextend", "devtools", "digest", "doParallel", "dplyr", "DT", "dtplyr", "dygraphs", "ellipse", "evaluate", "factoextra", "FactoMineR", "fansi", "fftwtools", "flexmix", "foreach", "foreign", "formatR", "Formula", "fpc", "gbm", "gdata", "gender", "geosphere", "ggplot2", "ggpubr", "ggrepel", "ggsci", "git2r", "gridExtra", "gtools", "highr", "Hmisc", "htmlTable", "htmlwidgets", "httpuv", "hunspell", "igraph", "inline", "irlba", "iterators", "janeaustenr", "kernlab", "knitr", "lattice", "lazyeval", "lme4", "lmtest", "loo", "mapproj", "maps", "maptools", "markdown", "MASS", "Matrix", "matrixStats", "mclust", "memoise", "mgcv", "mime", "miniUI", "modeltools", "multcomp", "munsell", "mvtnorm", "nloptr", "NLP", "NMF", "OAIHarvester", "openNLPdata", "openssl", "packrat", "pbkrtest", "pkgmaker", "PKI", "plotrix", "ps", "psych", "qdap", "qdapDictionaries", "qdapRegex", "qdapTools", "quantreg", "R6", "randomForest", "RANN", "raster", "Rcpp", "RcppEigen", "RCurl", "registry", "remotes", "reprex", "reshape2", "rgdal", "rgeos", "RgoogleMaps", "rJava", "rjson", "rlang", "rmarkdown", "rngtools", "robustbase", "rpart", "rpart.plot", "rprojroot", "rsconnect", "rscopus", "rstantools", "rstudioapi", "sandwich", "scales", "scatterplot3d", "selectr", "servr", "shiny", "shinyjs", "shinystan", "sm", "sourcetools", "sp", "SparseM", "statmod", "stringdist", "stringi", "stringr", "survival", "syuzhet", "testthat", "TH.data", "threejs", "tidyr", "tidyselect", "tidytext", "tm", "tokenizers", "topicmodels", "trimcluster", "viridis", "WDI", "withr", "wordcloud", "xlsx", "XML", "xml2", "xtable", "xts", "yaml", "zoo"))
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}y
install.packages("pacman")
library(packman)
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
install.packages("devtools")
install.packages("devtools")
install.packages("yaml")
install.packages(c("bit", "cairoDevice", "caTools", "chron", "curl", "data.table", "digest", "dplyr", "fansi", "foreign", "gbm", "ggplot2", "ggrepel", "git2r", "gtools", "igraph", "irlba", "kernlab", "lattice", "lme4", "lmtest", "mapproj", "maps", "maptools", "MASS", "Matrix", "matrixStats", "mclust", "mgcv", "mime", "mvtnorm", "nloptr", "NMF", "openssl", "ps", "quantreg", "randomForest", "RANN", "raster", "Rcpp", "RcppEigen", "RCurl", "rgdal", "rgeos", "rJava", "rjson", "rlang", "robustbase", "rstan", "rstanarm", "scales", "sentimentr", "slam", "sm", "sourcetools", "sp", "StanHeaders", "stringdist", "stringi", "stringr", "survival", "testthat", "tidyr", "tidyselect", "tm", "tokenizers", "wordcloud", "XML", "xml2", "xts", "zoo"))
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
install.packages("devtools")
library(devtools)
install.packages("base64enc")
install.packages("devtools")
install.packages("processx")
install.packages("devtools")
install.packages("backports")
install.packages("devtools")
install.packages("glue")
install.packages("devtools")
library(devtools)
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #97 files in total
### only process one of the files in the folder as an example, in your project, you need to use all the files
current_file_name <- sub(".txt","",file_name_vec[3])
## read the ground truth text
current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
files <- list.files(path="../data/ground_truth/", pattern="*.txt", full.names=TRUE, recursive=FALSE)
groundTruth <- ""
lapply(files, function(x) {
groundTruth <- cat(groundTruth, readChar(x))
})
lapply(files, function(x) {
groundTruth <- cat(groundTruth, readChar(x, file.info(x)$size))
})
groundTruth
for( x in files) groundTruth <- cat(groundTruth, readChar(x, file.info(x)$size))
groundTruth
for(x in files) groundTruth <- cat(groundTruth, readChar(x))
readChar(x, file.info(x)$size)
for(x in files) groundTruth <- paste(groundTruth, readChar(x, file.info(x)$size))
groundTruth
bag <- str_split(groundTruth," ")
library(stringr)
bag <- str_split(groundTruth," ")
bag
library(stringr)
groundTruth <- strsplit(groundTruth,"\n")[[1]]
groundTruth
groundTruth <- groundTruth[groundTruth!=""]
groundTruth
bag <- str_split(groundTruth," ")
bag
corpus <- VCorpus(VectorSource(bag))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
library(tm)
install.packages("tm")
library(tm)
corpus <- VCorpus(VectorSource(bag))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
corpus
head(corpus)
corpus$`1`
corpus$`137`
VectorSource(bag)
VCorpus(VectorSource(bag))
corpus[1]
corpus[1]$`1`
corpus %>% select(text)
library(tidyverse)
library(tidyselect)
corpus %>% select(text)
install.packages("dplyr")
library(dplyr)
corpus %>% select(text)
class(corpus)
corpus %>% tidy() %>% select(text)
library(tidyverse)
install.packages("tidyverse")
corpus %>% tidy() %>% select(text)
library(tidyverse)
corpus %>% tidy() %>% select(text)
library(broom)
corpus %>% tidy() %>% select(text)
stemmed <- tm_map(corpus, stemDocument) %>%
tidy() %>%
select(text)
install.packages("SnowballC")
stemmed <- tm_map(corpus, stemDocument) %>%
tidy() %>%
select(text)
dict <- tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
dict <- tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
bag
groundTruth
lapply(paste(groundTruth))
lapply(groundTruth, function(x) paste(x))
sapply(groundTruth, function(x) paste(x))
groundTruth <- sapply(groundTruth, function(x) paste(x))
bag <- str_split(groundTruth," ")
bag
unlist(bag)
bag <- unlist(bag)
corpus <- VCorpus(VectorSource(bag))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
dict <- tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
corpus
corpus[1]
bag
dict <- tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
dict <- tidyverse::tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
library(tidytext)
install.packages("tidytext")
library(tidytext)
dict <- tidytext::tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
dict2 <- bag %>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
head(dict)
dict
dict[1:30]
dict[,1:30]
dict[1:30,]
df %>% dict %>% print(n=40)
dict %>% tbl_df %>% print(n=40)
save(dict, "../output/dict.RData")
save(dict, file = "../output/dict.RData")
load("../output/dict.RData")
length(dict)
dict <- as.vector(dict)
head(dict)
dict <- as.data.frame(dict)
dict
head(dict)
head(dict, 40)
head(dict, 40:100)
head(dict, 100)
dict <- as.vector(dict)
dict
head(dict)
class(dict)
dict <- as.matrix(dict)
class(dict)
head(dict)
dict[order(nchar(dict), dict)]
dict <- dict[order(nchar(dict), dict)]
dict <- dict[nchar(dict) > 1]
head(dict)
tail(dict)
tail(dict, 50)
tail(dict, 1000)
dict <- dict[unique(dict)]
tail(dict, 1000)
load("../output/dict.RData")
dict <- as.matrix(dict)
head(dict)
dict <- dict[nchar(dict) > 1]
head(dict)
unique(dict)
dict <- unique(dict)
head(dict)
for(i in 1:length(dict))
strsplit(dict[1])
strsplit(dict[1], 1)
strsplit(dict[1], nchar(dict[1]))
